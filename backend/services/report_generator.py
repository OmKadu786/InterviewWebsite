from models.interview_schema import (
    InterviewReport, QuestionAnswer, VisionMetrics, NLPMetrics, EmotionData
)
from datetime import datetime
from typing import Dict, Any, List

def generate_report(session_data: Dict[str, Any], user_id: str) -> InterviewReport:
    """
    Constructs a comprehensive InterviewReport from the active session data.
    """
    state = session_data.get("interview_state")
    transcript = session_data.get("transcript", [])
    video_metrics = session_data.get("video_metrics", [])
    answer_scores = session_data.get("answer_scores", [])
    
    # 1. Process Vision Metrics
    emotions_list = []
    eye_contact_sum = 0
    steadiness_sum = 0
    count = len(video_metrics)
    
    emotion_counts = {}
    
    for m in video_metrics:
        # Map emotion index to string (assuming 0-6 mapping from video_service or similar)
        # For now, we use a placeholder or the raw value if it's already a string
        # Looking at video_service.py might clarify, but let's assume it returns an int or string.
        # In main.py video_websocket: result["emotion"] is stored.
        # Let's assume it's a string or mapped int.
        # If it's an int, we should map it. But let's assume it's handled.
        emotion = str(m["emotion"]) 
        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        emotions_list.append(EmotionData(
            timestamp=m["timestamp"],
            emotion=emotion,
            confidence=m["confidence"]
        ))
        eye_contact_sum += m["focus"]
        # "confidence" in video metrics is used as steadiness/confidence score
        steadiness_sum += m["confidence"]

    avg_eye_contact = eye_contact_sum / count if count > 0 else 0
    avg_steadiness = steadiness_sum / count if count > 0 else 0
    most_common_emotion = max(emotion_counts, key=emotion_counts.get) if emotion_counts else "Unknown"

    vision_metrics_obj = VisionMetrics(
        eye_contact_percentage=avg_eye_contact,
        steadiness_score=avg_steadiness,
        emotions=emotions_list,  # Might need to sample this if too large
        average_emotion=most_common_emotion
    )
    
    # 2. Process NLP Metrics (Simplified)
    user_msgs = [t["content"] for t in transcript if t["role"] == "user"]
    total_words = sum(len(m.split()) for m in user_msgs)
    avg_len = total_words / len(user_msgs) if user_msgs else 0
    
    # Filler detection (basic)
    fillers = ["um", "uh", "like", "you know"]
    filler_count = sum(m.lower().count(f) for m in user_msgs for f in fillers)
    
    nlp_metrics_obj = NLPMetrics(
        filler_word_count=filler_count,
        average_response_length=int(avg_len),
        vocabulary_richness=0.0, # Placeholder, requires more NLP
        coherence_score=0.0      # Placeholder
    )
    
    # 3. Process Questions & Answers
    qa_list = []
    total_semantic_score = 0
    
    if state and state.answer_scores:
        for idx, score_data in enumerate(state.answer_scores):
            # Calculate a semantic score 0-100 based on accuracy, depth, clarity
            # score_data has "scores": {"accuracy": X, "depth": Y, "clarity": Z}
            s = score_data["scores"]
            avg_s = (s["accuracy"] + s["depth"] + s["clarity"]) / 3 * 10
            total_semantic_score += avg_s
            
            qa_list.append(QuestionAnswer(
                question_number=idx + 1,
                question=score_data["question"],
                expected_answer="", # Not stored in state currently
                user_answer=score_data["answer"],
                semantic_score=avg_s,
                feedback=f"Accuracy: {s['accuracy']}/10, Depth: {s['depth']}/10",
                timestamp=datetime.utcnow() # Placeholder
            ))
    
    overall_score = total_semantic_score / len(qa_list) if qa_list else 0
    
    # 4. Construct Report
    report = InterviewReport(
        user_id=user_id,
        interview_date=datetime.utcnow(),
        job_role="Software Engineer", # Should come from job_description analysis or user input
        company="Unknown",
        interview_duration=len(transcript) * 30, # Approx duration
        resume_filename="resume.pdf",
        job_description=session_data.get("job_description"),
        questions_answers=qa_list,
        total_questions=len(qa_list),
        overall_semantic_score=overall_score,
        vision_metrics=vision_metrics_obj,
        nlp_metrics=nlp_metrics_obj,
        confidence_timeline=[], # Can be built from video metrics
        emotion_timeline=[],    # Can be built from emotions_list
        strengths=[],           # Needs LLM summary
        areas_for_improvement=[], # Needs LLM summary
        overall_feedback="Generated automatically.",
        interview_number=1     # Needs DB lookup to find Nth interview
    )
    
    # Add optional fields from State if available
    if state:
        report.hint_usage = state.hint_usage
        report.logical_errors = state.logical_errors
        report.question_topics = state.question_topics
        # report.time_per_question = state.question_timestamps # Type mismatch possible, map carefully
        
    return report
