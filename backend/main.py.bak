import os
import json
import base64
import time
import cv2
import threading
from fastapi import FastAPI, WebSocket, UploadFile, File, Form
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
from dotenv import load_dotenv

# Import fixed: using the new function name to avoid ImportError
from services.pdf_service import extract_text_from_pdf
from services.llm_service import get_ai_response, get_hint
from services.tts_service import generate_audio
from services.video_service import process_video_frame, Stabilizer

load_dotenv()

app = FastAPI()
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    return {"message": "Backend running"}


# Global camera state
camera = None
camera_lock = threading.Lock()
camera_active = False

session_data = {
    "resume_text": "", 
    "job_description": "",
    "transcript": [],     # Stores {"role": "user"|"ai", "content": "..."}
    "video_metrics": []   # Stores {"timestamp": float, "focus": int, "emotion": int, "confidence": int}
}

class HintRequest(BaseModel):
    question: str
    level: str = "medium"  # small, medium, or full

@app.post("/get-hint")
async def get_interview_hint(request: HintRequest):
    if not session_data["resume_text"]:
        return {"hint": "Please upload a resume first."}
    
    hint = await get_hint(request.question, session_data["resume_text"], session_data["job_description"], request.level)
    return {"hint": hint}

@app.post("/upload-resume")
async def upload_resume(file: UploadFile = File(...), job_description: str = Form(...)):
    pdf_bytes = await file.read()
    text = extract_text_from_pdf(pdf_bytes)
    session_data["resume_text"] = text
    session_data["job_description"] = job_description
    session_data["transcript"] = []     # Reset on new upload
    session_data["video_metrics"] = []  # Reset on new upload
    return {"message": "Data processed successfully!"}

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    temp_path = "temp_voice.wav"
    with open(temp_path, "wb") as buffer:
        buffer.write(await file.read())
    
    with open(temp_path, "rb") as audio_file:
        transcript = await client.audio.transcriptions.create(
            model="whisper-1", 
            file=audio_file,
            language="en", # Forces English to stop the Korean hallucinations
            prompt="Technical interview conversation about software development." # Contextual hint
        )
    
    os.remove(temp_path)
    
    # Filter out "hallucinations" (very short or nonsense noise)
    text = transcript.text.strip()
    if len(text) < 2: 
        return {"text": ""} # Return empty so the AI doesn't reply to a 'noise' message
        
    return {"text": text}

@app.websocket("/ws/interview")
async def interview_websocket(websocket: WebSocket):
    await websocket.accept()
    
    # Track history for this specific connection
    chat_history = []
    
    # 1. Automatic Greeting
    if session_data["resume_text"]:
        response_text = await get_ai_response(
            session_data["resume_text"], 
            session_data["job_description"],
            chat_history
        )
        
        chat_history.append({"role": "assistant", "content": response_text})
        audio_bytes = generate_audio(response_text)
        if audio_bytes:
            audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
        else:
            audio_b64 = None
        
        await websocket.send_json({
            "type": "ai_turn",
            "text": response_text,
            "audio": audio_b64
        })

    # 2. Conversation Loop
    try:
        while True:
            data = await websocket.receive_text()
            msg = json.loads(data)
            
            if msg["type"] == "user_turn":
                # Add user message to history
                chat_history.append({"role": "user", "content": msg["text"]})
                session_data["transcript"].append({"role": "user", "content": msg["text"]})
                
                # Get next question/response from AI
                ai_reply = await get_ai_response(
                    session_data["resume_text"],
                    session_data["job_description"],
                    chat_history
                )
                
                chat_history.append({"role": "assistant", "content": ai_reply})
                session_data["transcript"].append({"role": "ai", "content": ai_reply})
                
                # Generate and send audio response
                audio_bytes = generate_audio(ai_reply)
                if audio_bytes:
                    audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
                else:
                    audio_b64 = None
                
                await websocket.send_json({
                    "type": "ai_turn",
                    "text": ai_reply,
                    "audio": audio_b64
                })
    except Exception as e:
        print(f"WebSocket closed or error: {e}")

@app.websocket("/ws/video")
async def video_websocket(websocket: WebSocket):
    await websocket.accept()
    stabilizer = Stabilizer()
    
    try:
        while True:
            data = await websocket.receive_text()
            # data is expected to be a base64 string
            result = process_video_frame(data, stabilizer)
            if result:
                # Store metric with timestamp
                metric_entry = {
                    "timestamp": time.time(),
                    "focus": result["focus"],
                    "emotion": result["emotion"],
                    "confidence": result["confidence"],
                    "stress": result["stress"]
                }
                session_data["video_metrics"].append(metric_entry)
                
                await websocket.send_json(result)
    except Exception as e:
        print(f"Video WebSocket error: {e}")

@app.get("/report")
async def get_report_data():
    """
    Returns the accumulated session data for the report page.
    Frontend will calculate averages and stats.
    """
    return {
        "transcript": session_data["transcript"],
        "video_metrics": session_data["video_metrics"],
        "job_description": session_data["job_description"]
    }

# ==================== WEBCAM STREAMING ENDPOINTS ====================

def generate_frames():
    """Generator function for MJPEG streaming"""
    global camera, camera_active
    
    with camera_lock:
        if camera is None:
            camera = cv2.VideoCapture(0)
            camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        camera_active = True
    
    try:
        while camera_active:
            with camera_lock:
                if camera is None:
                    break
                success, frame = camera.read()
            
            if not success:
                break
            
            # Encode frame as JPEG
            ret, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 80])
            if not ret:
                continue
            
            frame_bytes = buffer.tobytes()
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            
            time.sleep(0.033)  # ~30 FPS
    except Exception as e:
        print(f"Stream error: {e}")

@app.get("/api/stream")
async def video_stream():
    """MJPEG video stream endpoint for frontend"""
    return StreamingResponse(
        generate_frames(),
        media_type="multipart/x-mixed-replace; boundary=frame"
    )

@app.api_route("/api/stop-camera", methods=["GET", "POST"])
async def stop_camera():
    """Release camera resources"""
    global camera, camera_active
    
    camera_active = False
    with camera_lock:
        if camera is not None:
            camera.release()
            camera = None
    
    return {"status": "Camera stopped"}

@app.get("/health")
def health():
    return {"status": "ok"}

# ==================== METRICS WEBSOCKET (ALIAS) ====================

@app.websocket("/ws/metrics")
async def metrics_websocket(websocket: WebSocket):
    """WebSocket for real-time video metrics - alias to /ws/video for frontend compatibility"""
    await video_websocket(websocket)

# ==================== ANALYTICS ENDPOINTS ====================

@app.get("/api/analytics")
async def get_analytics():
    """
    Returns structured analytics data for the CandidateReport component.
    Aggregates video_metrics and transcript into the expected format.
    """
    metrics = session_data["video_metrics"]
    transcript = session_data["transcript"]
    
    # Calculate averages from video metrics
    if metrics:
        avg_focus = sum(m["focus"] for m in metrics) / len(metrics)
        avg_emotion = sum(m["emotion"] for m in metrics) / len(metrics)
        avg_confidence = sum(m["confidence"] for m in metrics) / len(metrics)
        avg_stress = sum(m.get("stress", 0) for m in metrics) / len(metrics)
    else:
        avg_focus = avg_emotion = avg_confidence = 0
        avg_stress = 50
    
    # Calculate per-question metrics (group by 30-second windows)
    per_question_metrics = []
    if metrics:
        # Group metrics into question-like segments
        segment_size = max(1, len(metrics) // 5)  # Divide into ~5 segments
        for i in range(0, len(metrics), segment_size):
            segment = metrics[i:i+segment_size]
            if segment:
                per_question_metrics.append({
                    "question_index": len(per_question_metrics) + 1,
                    "eye_contact_percentage": sum(m["focus"] for m in segment) / len(segment),
                    "confidence": sum(m["confidence"] for m in segment) / len(segment)
                })
    
    # Sentiment trend from transcript (simplified)
    sentiment_trend = []
    for entry in transcript:
        if entry["role"] == "user":
            # Basic sentiment: positive words = +, negative words = -
            text = entry["content"].lower()
            positive_words = ["good", "great", "excellent", "love", "happy", "excited", "confident"]
            negative_words = ["bad", "difficult", "hard", "nervous", "worried", "afraid", "confused"]
            pos_count = sum(1 for w in positive_words if w in text)
            neg_count = sum(1 for w in negative_words if w in text)
            sentiment = (pos_count - neg_count) / max(1, pos_count + neg_count + 1)
            sentiment_trend.append(sentiment)
    
    # Filler word detection
    filler_words = ["um", "uh", "like", "you know", "basically", "actually", "literally"]
    total_fillers = 0
    filler_counts = {}
    total_words = 0
    
    for entry in transcript:
        if entry["role"] == "user":
            words = entry["content"].lower().split()
            total_words += len(words)
            for filler in filler_words:
                count = entry["content"].lower().count(filler)
                total_fillers += count
                filler_counts[filler] = filler_counts.get(filler, 0) + count
    
    most_common_fillers = sorted(filler_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    filler_rate = (total_fillers / max(1, total_words)) * 100
    
    # Talk-to-listen ratio
    user_messages = sum(1 for t in transcript if t["role"] == "user")
    ai_messages = sum(1 for t in transcript if t["role"] == "ai")
    talk_ratio = user_messages / max(1, ai_messages)
    
    return {
        "radar_chart_data": {
            "technical_accuracy": min(100, avg_confidence + 15),  # Derived metric
            "communication": min(100, avg_emotion + 20),
            "confidence": avg_confidence,
            "focus": avg_focus,
            "emotional_intelligence": avg_emotion
        },
        "vision_analytics": {
            "overall_eye_contact_percentage": avg_focus,
            "overall_steadiness_percentage": 100 - avg_stress,
            "per_question_metrics": per_question_metrics
        },
        "nlp_report": {
            "total_filler_count": total_fillers,
            "filler_rate": filler_rate,
            "talk_to_listen_ratio": talk_ratio,
            "most_common_fillers": most_common_fillers,
            "sentiment_trend": sentiment_trend
        },
        "scoring_summary": {
            "average_score": (avg_focus + avg_emotion + avg_confidence) / 3,
            "scores_over_time": [m["confidence"] for m in metrics[-10:]]  # Last 10 readings
        }
    }
    
@app.get("/api/analytics/timeline")
async def get_analytics_timeline():
    """Returns time-series data for charts"""
    metrics = session_data["video_metrics"]
    return {
        "timeline": [
            {
                "timestamp": m["timestamp"],
                "focus": m["focus"],
                "emotion": m["emotion"],
                "confidence": m["confidence"]
            }
            for m in metrics
        ]
    }
    
@app.post("/api/analytics/feedback")
async def generate_analytics_feedback():
    """Generate AI-powered feedback based on session data"""
    metrics = session_data["video_metrics"]
    transcript = session_data["transcript"]
    
    # Calculate summary stats
    if metrics:
        avg_focus = sum(m["focus"] for m in metrics) / len(metrics)
        avg_emotion = sum(m["emotion"] for m in metrics) / len(metrics)
        avg_confidence = sum(m["confidence"] for m in metrics) / len(metrics)
    else:
        avg_focus = avg_emotion = avg_confidence = 50
    
    # Generate strengths based on metrics
    strengths = []
    if avg_focus >= 70:
        strengths.append("Excellent eye contact maintained throughout the interview")
    if avg_emotion >= 60:
        strengths.append("Good emotional expression and engagement")
    if avg_confidence >= 70:
        strengths.append("Displayed strong confidence in responses")
    if len(transcript) >= 6:
        strengths.append("Active participation with thorough responses")
    
    if not strengths:
        strengths.append("Completed the interview session")
    
    # Generate improvements based on metrics
    improvements = []
    if avg_focus < 70:
        improvements.append("Work on maintaining consistent eye contact with the camera")
    if avg_emotion < 60:
        improvements.append("Try to show more enthusiasm and expressiveness")
    if avg_confidence < 70:
        improvements.append("Practice to build more visible confidence in responses")
    if len(transcript) < 6:
        improvements.append("Provide more detailed answers to demonstrate depth of knowledge")
    
    if not improvements:
        improvements.append("Continue practicing to maintain your excellent performance")
    
    return {
        "strengths": strengths,
        "improvements": improvements
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
